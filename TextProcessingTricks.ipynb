{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextProcessingTricks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOdHDbrQbMoPPW5CdpLsJu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVFznNPomqW",
        "outputId": "094f07a8-2b46-43d0-9c0f-d61f33bb8195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'No': 1, 'he': 2, 'hecho': 3, 'tareas': 4, 'del': 5, 'colegio': 6, 'porque': 7, 'me': 8, 'suda': 9, 'el': 10, 'rabo': 11, 'Arepa': 12, 'rellena': 13}\n"
          ]
        }
      ],
      "source": [
        "# To start with this topic, is very important understand the n-grams method, so, we will see how it works in simple exercise.\n",
        "\n",
        "import numpy as np # To tokenization of sentences.\n",
        "\n",
        "samples = ['No he hecho tareas del colegio porque me suda el rabo', 'Arepa rellena']\n",
        "\n",
        "token_index = {} # Dict to all index tokens.\n",
        "\n",
        "\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1 # len(token_index) + 1 represents the ID \n",
        "\n",
        "\n",
        "print(token_index) # One-Hot encoding (n-bag tokenization, where n is the number of words in samples list)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 10 # idk.\n",
        "\n",
        "\n",
        "results = np.zeros(shape = (len(samples), \n",
        "                   max_length, max(token_index.values()) + 1 )) # We crate an array of zeros.\n",
        "\n",
        "# print(results.shape) # (2, 10, 14).\n",
        "\n",
        "\n",
        "\n",
        "for i, sample in  enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length:]: # In each sentence, We get only the first 10 words (from index 0)\n",
        "    index = token_index.get(word) # Get index.\n",
        "    results[i, j, index] = 1\n",
        "    \n",
        "\n",
        "\n",
        "# print(results) # Return numpy array."
      ],
      "metadata": {
        "id": "mzR01RGtWyXs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can this do this to tokenize characters. We will try.\n",
        "\n",
        "import string # Standard library from python.\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "\n",
        "\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "print(token_index) # Set index to dict of characters.\n",
        "\n",
        "max_length = 50\n",
        "\n",
        "\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1)) # We create a numpy array.\n",
        "print(results.shape)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec9zD6mDgl_D",
        "outputId": "5b85fee2-6a94-43dc-fce5-a2f65ecd93ed"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', 41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', 51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', 61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', 71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', 81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', 91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'}\n",
            "(2, 50, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Also we will do this with help from keras. Everything is most easier with this."
      ],
      "metadata": {
        "id": "kbS2stfCtPlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer # The tokenizator from Keras.\n",
        "\n",
        "\n",
        "samples = ['No he hecho tareas del colegio porque me suda el rabo', 'Arepa rellena']\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 1000) # The tokenizator take the 1000 words most commons into dataset.\n",
        "tokenizer.fit_on_texts(samples) # Building the word index.\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples) # Change each word in list to integer indices.\n",
        "print(sequences) \n",
        "\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary') # Convert the text to matrix. With dis we will make a model train.\n",
        "print(one_hot_results)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index) # We built a word index (dictionary) most easy with Keras. The same.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suf-0J3QuzWu",
        "outputId": "4e1352a7-47d9-4689-c19a-a53f8af02f75"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13]]\n",
            "[[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "{'no': 1, 'he': 2, 'hecho': 3, 'tareas': 4, 'del': 5, 'colegio': 6, 'porque': 7, 'me': 8, 'suda': 9, 'el': 10, 'rabo': 11, 'arepa': 12, 'rellena': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding (to word level) with hashing trick.\n",
        "samples = ['Hoy iré a comprarme una empanada ranchera.', 'Mi papá se fué a comprar cigarros.']\n",
        "\n",
        "dimensionality = 1000 # Variable to set array dimension. If we have lower dimensionality (or our words index closer to 1000),\n",
        "# it's very likely that the accuracy of this method will decrease considerably.\n",
        "\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length:]:\n",
        "    index = abs(hash(word)) % dimensionality # We hash word to radom integer index (from 0 to 1000, in this case).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAFB9HwW3C2C",
        "outputId": "51ef13a8-faa5-4569-8ba9-99f05c35bce4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "821\n",
            "993\n",
            "266\n",
            "557\n",
            "621\n",
            "936\n",
            "146\n",
            "205\n",
            "321\n",
            "741\n",
            "188\n",
            "266\n",
            "597\n",
            "249\n"
          ]
        }
      ]
    }
  ]
}